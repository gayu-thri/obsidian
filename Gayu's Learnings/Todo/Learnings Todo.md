- HuggingFace NLP Tutorial course 
	- https://huggingface.co/learn/nlp-course/chapter1/1
	- Want list from the course:
		1. Tokenizer training
		2. Text generation using BERT
		3. Causal & Masked LM
- Transformer complete basics
	- https://jalammar.github.io/illustrated-transformer/
	- Want list from above:
		1. Attention mechanism (K, Q, V) with proper example
		2. Positional Embeddings
- Fairseq complete flow in their official documentation
	- https://repository.zoho.com/zohocorp/tp/fairseq#/source/default/fairseq/examples
		- Specific example: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md
---
### Immediate Todos

DPO  
Architectures used for LLMs  
- [https://www.labellerr.com/blog/exploring-architectures-and-configurations-for-large-language-models-llms/amp/](https://www.labellerr.com/blog/exploring-architectures-and-configurations-for-large-language-models-llms/amp/)  
- [https://whylabs.ai/learning-center/introduction-to-llms/understanding-large-language-model-architectures](https://whylabs.ai/learning-center/introduction-to-llms/understanding-large-language-model-architectures)  
Floating point operations (FLoPs)  
Mixed precision (fp16, fp32)  
Optimiser (Adam, Adafactor)  
RoPE  
SwiGLU activation function  
Gradient checkpoints  
  
Code  
- Fine-tuning BERT or GPT-like models (HuggingFace NLP Course)  
- [https://huggingface.co/learn/nlp-course/en/chapter7/6](https://huggingface.co/learn/nlp-course/en/chapter7/6)  
- [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)(Edited)

