#### Model
- Understanding
	- [Understanding LLMs: A Comprehensive Overview from Training to Inference](https://arxiv.org/pdf/2401.02038.pdf)
	- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
	- [InternLM2 Technical Report](https://arxiv.org/pdf/2403.17297.pdf)
- Models
	- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
	- [Generative Pre-trained Transformer (_GPT_)](https://arxiv.org/abs/2305.10435)
	- [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/pdf/2306.02707.pdf)
	- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)

#### Dataset
- [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/pdf/2101.00027.pdf)
- [CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data](https://arxiv.org/pdf/1911.00359.pdf)
- RedPajama

#### Filtering
- [QuRating: Selecting High-Quality Data for Training Language Models](https://arxiv.org/pdf/2402.09739.pdf)

#### Repositories
- https://github.com/Hannibal046/Awesome-LLM